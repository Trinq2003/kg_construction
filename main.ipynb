{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wallace/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from knowledge_graph.modules.document.document_layer import DocumentLayer\n",
    "from knowledge_graph.modules.document.document_handler import DocumentHandler\n",
    "from knowledge_graph.modules.entity.er_extractor import ERExtractor\n",
    "from knowledge_graph.modules.entity.entity_layer import EntityLayer\n",
    "from knowledge_graph.modules.node import EntityNode, ContentNode\n",
    "from configuration.configurations import ERExtractorConfiguration\n",
    "from configuration.llm_inference_configuration import APILLMConfiguration, LocalLLMConfiguration\n",
    "from configuration.embedding_inference_configuration import APIEmbeddingModelConfiguration, LocalEmbeddingModelConfiguration\n",
    "from llm.language_models.azure_gpt import AzureGPT\n",
    "from llm.language_models.hf_local_model import HuggingfaceLocalInference\n",
    "from embedding.embedding_models.hf_embedding import HFLocalEmbeddingModel \n",
    "from exception.entity_exception import EntityDuplicationInOneContentNodeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local embedding models\n",
    "emb_config = LocalEmbeddingModelConfiguration()\n",
    "emb_config.load(path='configuration/toml/intfloat_ml_e5.toml')\n",
    "emb_model = HFLocalEmbeddingModel(embedding_model_config=emb_config)\n",
    "\n",
    "# Local LLM models\n",
    "triplet_extraction_llm_config = LocalLLMConfiguration()\n",
    "triplet_extraction_llm_config.load(path='configuration/toml/triplex.toml')\n",
    "triplet_extraction_llm = HuggingfaceLocalInference(llm_config=triplet_extraction_llm_config)\n",
    "\n",
    "conference_resolution_llm_config = LocalLLMConfiguration()\n",
    "conference_resolution_llm_config.load(path='configuration/toml/gemma2_ft.toml')\n",
    "conference_resolution_llm = HuggingfaceLocalInference(llm_config=conference_resolution_llm_config)\n",
    "\n",
    "# API LLM models\n",
    "judgement_llm_config = APILLMConfiguration()\n",
    "judgement_llm_config.load(path='configuration/toml/gpt_4o.toml')\n",
    "judgement_llm = AzureGPT(llm_config=judgement_llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated field from a procedure. ('advertisedListenAddress' returned by 'gds.debug.arrow' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: 'CALL gds.debug.arrow()'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated field from a procedure. ('serverLocation' returned by 'gds.debug.arrow' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: 'CALL gds.debug.arrow()'\n"
     ]
    }
   ],
   "source": [
    "emb_config = EmbeddingModelConfiguration()\n",
    "er_config = ERExtractorConfiguration()\n",
    "azure_gpt_config = APILLMConfiguration()\n",
    "kg_config = Neo4jConfiguration()\n",
    "emb_config.load(\"configuration/toml/hf_embedding.toml\")\n",
    "er_config.load(\"configuration/toml/extractor.toml\")\n",
    "azure_gpt_config.load(\"configuration/toml/azure_openai.toml\")\n",
    "kg_config.load(\"configuration/toml/neo4j.toml\")\n",
    "\n",
    "emb_model = HFEmbeddingModel(embedding_model_config=emb_config)\n",
    "azure_gpt = AzureGPT(azure_gpt_config)\n",
    "entity_extractor = ERExtractor(azure_gpt, er_config)\n",
    "\n",
    "knowledge_graph = KnowledgeGraph(kg_config)\n",
    "graph_ds = KnowledgeGraphDataScience(kg_config)\n",
    "node_matcher = NodeMatcher(knowledge_graph)\n",
    "rel_matcher = RelationshipMatcher(knowledge_graph)\n",
    "\n",
    "document_layer = DocumentLayer(knowledge_graph)\n",
    "document_layer.load_embedding_model(emb_model)\n",
    "entity_layer = EntityLayer(knowledge_graph)\n",
    "entity_layer.load_embedding_model(emb_model)\n",
    "entity_layer.load_graph_ds(graph_ds)\n",
    "community_layer = CommunityLayer(knowledge_graph)\n",
    "community_layer.load_node_matcher(node_matcher)\n",
    "community_layer.load_rel_matcher(rel_matcher)\n",
    "community_layer.load_llm(azure_gpt)\n",
    "community_layer.load_embedding_model(emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_DIRPATH = r\"data/processed/md/test copy\"\n",
    "\n",
    "document_node_1 = DocumentNode(document_id=\"1\", title=\"draft_1\", version=\"1.0\", last_update=str(datetime.now()), attached_to=[], document_directory=\"\")\n",
    "document_node_2 = DocumentNode(document_id=\"2\", title=\"draft_2\", version=\"1.0\", last_update=str(datetime.now()), attached_to=[\"1\"], document_directory=\"\")\n",
    "document_node_3 = DocumentNode(document_id=\"3\", title=\"draft_3\", version=\"1.0\", last_update=str(datetime.now()), attached_to=[\"1\"], document_directory=\"\")\n",
    "document_node_4 = DocumentNode(document_id=\"4\", title=\"draft_4\", version=\"1.0\", last_update=str(datetime.now()), attached_to=[\"1\"], document_directory=\"\")\n",
    "document_node_5 = DocumentNode(document_id=\"5\", title=\"draft_5\", version=\"1.0\", last_update=str(datetime.now()), attached_to=[\"3\"], document_directory=\"\")\n",
    "document_node_6 = DocumentNode(document_id=\"6\", title=\"test copy\", version=\"1.0\", last_update=str(datetime.now()), attached_to=[], document_directory=DOCUMENT_DIRPATH)\n",
    "\n",
    "# document_layer.add_document_node_to_graph(document_node_1)\n",
    "# document_layer.add_document_node_to_graph(document_node_2)\n",
    "# document_layer.add_document_node_to_graph(document_node_3)\n",
    "# document_layer.add_document_node_to_graph(document_node_4)\n",
    "# document_layer.add_document_node_to_graph(document_node_5)\n",
    "# document_layer.add_document_node_to_graph(document_node_6)\n",
    "\n",
    "# document_layer.create_document_tree(document_node=document_node_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_layer.load_er_extractor(entity_extractor)\n",
    "\n",
    "content_nodes = document_layer.get_content_nodes_by_document(document_node_6)\n",
    "\n",
    "for content_node in content_nodes:\n",
    "    entity_layer.er_process_content_node(content_node=content_node, num_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_entities = entity_layer.find_similar_entity_nodes_in_given_document_node(document_node=document_node_6)\n",
    "similar_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in similar_entities:\n",
    "    entity_layer.merge_entity_nodes_from_id_list(entity_node_ids=group, forced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_layer.entity_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_layer.create_community_nodes()\n",
    "community_layer.community_ranks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_community_nodes = community_layer.get_community_nodes()\n",
    "community_nodes = [CommunityNode(**raw_community_node) for raw_community_node in _community_nodes]\n",
    "\n",
    "for community_node in [CommunityNode(**raw_community_node) for raw_community_node in community_nodes]:\n",
    "    community_layer.community_node_info_aggregation(community_node=community_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What happened to deep learning at 1990s?\"\n",
    "user_query_embedding = emb_model.encode(user_query)\n",
    "top_k_retrieval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1-4': 'Deep learning, a subset of machine learning that utilizes neural networks with multiple layers, has seen significant advancements thanks to the use of graphical processing units (GPUs). Initially developed to accelerate graphics processing for computer games, GPUs are optimized for high throughput matrix-vector products. This optimization has proven to be a game changer, making deep learning tasks feasible and more efficient.',\n",
       " '1-9': 'Traditional methods in data science, such as linear and kernel methods, are grounded in convex optimizations. Linear methods and kernel methods are specific types of traditional methods that leverage these mathematical techniques to solve various problems. However, with the advent of deep models, which are neural networks with many layers, there has been a significant shift in performance capabilities. When provided with large amounts of data, deep models can substantially outperform traditional methods, showcasing their advanced capabilities in handling complex data-driven tasks.',\n",
       " '1-6': 'In the 1990s, the field of computational research predominantly relied on simpler algorithms due to their efficient optimization of convex objectives. This preference was driven by the limited storage capacity, expensive sensors, and tighter research budgets of the time, which necessitated the use of smaller datasets. The UCI collection of datasets, a repository containing hundreds or thousands of low-resolution images with artificially clean backgrounds, was heavily utilized during this period.\\n\\nAs the industry transitioned into the early 2000s, the preference for simple algorithms persisted. These computational methods continued to be favored for their ability to efficiently optimize convex objectives, maintaining their relevance in research and application. This era marked a continuation of the trends established in the previous decade, reflecting the ongoing constraints and technological limitations of the time.',\n",
       " '1-5': 'In 2012, researchers Alex Krizhevsky and Ilya Sutskever implemented a groundbreaking deep convolutional neural network (CNN) that significantly contributed to the deep learning boom. This deep CNN, which utilized two NVIDIA GTX 580 GPUs each with 3GB of memory and capable of 1.5 TFLOPs, was notable for its ability to run on GPUs, enabling faster computations. The network involved complex operations such as matrix multiplications and fast convolutions, which were parallelized in hardware to enhance performance. The implementation of this deep CNN marked a pivotal moment in the field of deep learning, driving significant advancements and widespread adoption of deep learning technologies in subsequent years. The collaboration between Krizhevsky and Sutskever was instrumental in achieving these technological milestones.',\n",
       " '1-1': 'Training deep learning models is a computationally intensive process that involves several key components and metrics. At the core of this process are linear algebra operations, which are essential computational methods used extensively during training. These operations involve complex matrix and vector computations that are repeated across many layers of the model.\\n\\nThe training process itself is defined by passing data through the model for hundreds of epochs, where an epoch represents a complete pass through the training dataset. This iterative process helps the model learn and improve its performance over time.\\n\\nDeep learning models, which are characterized by their multiple layers and complex structures, require significant compute cycles to perform these operations. Compute cycles refer to the processing power needed to execute the computations, and deep learning models are known to consume a large amount of these cycles during training.\\n\\nIn summary, training deep learning models involves performing numerous linear algebra operations over many epochs, consuming substantial compute cycles to achieve the desired learning outcomes.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve top-k community nodes based on user query\n",
    "level = 1\n",
    "community_nodes_by_level = node_matcher.match(\"__Community__\", level=level).all()\n",
    "\n",
    "similar_summary_comparison = [user_query_embedding@community_node['vector_emb'] for community_node in community_nodes_by_level]\n",
    "top_k_community_nodes = sorted(zip(similar_summary_comparison, community_nodes_by_level), key=lambda x: x[0], reverse=True)[:top_k_retrieval]\n",
    "\n",
    "top_k_community_nodes_id_content = {}\n",
    "for community_node in top_k_community_nodes:\n",
    "    community_node_id = community_node[1]['community_id']\n",
    "    community_node_content = community_node[1]['summary_content']\n",
    "    top_k_community_nodes_id_content[community_node_id] = community_node_content\n",
    "\n",
    "top_k_community_nodes_id_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cgitb import text\n",
    "import json\n",
    "from nlp.basic import string_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Embedding similarity function using cosine similarity\n",
    "def embedding_similarity(embedding1: list, embedding2: list) -> float:\n",
    "    # Ensure the embeddings are numpy arrays\n",
    "    emb1 = np.array(embedding1).reshape(1, -1)\n",
    "    emb2 = np.array(embedding2).reshape(1, -1)\n",
    "\n",
    "    # Compute the cosine similarity between the two embeddings\n",
    "    similarity = cosine_similarity(emb1, emb2)\n",
    "\n",
    "    return similarity[0][0]  # Cosine similarity returns a 2D array, so we take the first value\n",
    "\n",
    "# Extract entities using GPT model\n",
    "entity_extraction_message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a specialist in linguistics, assisting with entity and relationship extraction for the construction of a knowledge graph.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Extract all the entities from the given text, output in JSON format ({{'entities': [List of extracted entities]}}): {user_query}\"}\n",
    "]\n",
    "raw_response = azure_gpt.chat(entity_extraction_message)\n",
    "text_response = azure_gpt.get_response_texts(raw_response)[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output = json.loads(text_response)\n",
    "extracted_entities = json_output['entities']\n",
    "\n",
    "# Retrieve all entity nodes from Neo4j graph\n",
    "raw_entity_nodes = node_matcher.match(\"__Entity__\").all()\n",
    "\n",
    "# Weights for name similarity and embedding similarity (you can adjust these based on importance)\n",
    "name_weight = 0.5\n",
    "embedding_weight = 0.5\n",
    "\n",
    "# Loop over extracted entities and find the most similar entity in the graph\n",
    "similar_entity_nodes = []\n",
    "\n",
    "for extracted_entity in extracted_entities:\n",
    "    best_match = None\n",
    "    best_combined_similarity = 0.0\n",
    "\n",
    "    # Assume the extracted entity has its own embedding (this can be created using the emb_model)\n",
    "    extracted_entity_embedding = emb_model.encode(extracted_entity)\n",
    "\n",
    "    for raw_entity_node in raw_entity_nodes:\n",
    "        entity_node = EntityNode(**raw_entity_node)\n",
    "        entity_name = entity_node.name\n",
    "        entity_name_embedding = entity_node.name_emb\n",
    "\n",
    "        # Calculate string similarity between the extracted entity and the entity node's name\n",
    "        name_similarity = string_similarity(extracted_entity, entity_name)\n",
    "\n",
    "        # Calculate embedding similarity between the extracted entity's embedding and the entity node's name embedding\n",
    "        embedding_similarity_score = embedding_similarity(extracted_entity_embedding, entity_name_embedding)\n",
    "\n",
    "        # Combine the two similarities using a weighted sum\n",
    "        combined_similarity = (name_similarity * name_weight) + (embedding_similarity_score * embedding_weight)\n",
    "\n",
    "        # Keep track of the entity node with the highest combined similarity score\n",
    "        if combined_similarity > best_combined_similarity:\n",
    "            best_combined_similarity = combined_similarity\n",
    "            best_match = entity_node\n",
    "\n",
    "    # Only add the match if the combined similarity score is above a certain threshold (e.g., 0.85)\n",
    "    if best_match and best_combined_similarity > 0.85:\n",
    "        similar_entity_nodes.append({\n",
    "            'extracted_entity': extracted_entity,\n",
    "            'similar_entity_node': best_match,\n",
    "            'combined_similarity_score': best_combined_similarity\n",
    "        })\n",
    "\n",
    "# Output the results\n",
    "for match in similar_entity_nodes:\n",
    "    print(f\"Extracted Entity: {match['extracted_entity']}\")\n",
    "    print(f\"Most Similar Entity in Graph: {match['similar_entity_node'].name}\")\n",
    "    print(f\"Combined Similarity Score: {match['combined_similarity_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_construction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
